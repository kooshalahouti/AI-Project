{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phase1: preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import library\n",
    "import pandas as pd\n",
    "import collections\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the punkt tokenizer if not already downloaded\n",
    "nltk.download('punkt')\n",
    "# Download the stopwords if not already downloaded\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14951</td>\n",
       "      <td>7627d7ca56</td>\n",
       "      <td>Hi : Found you through . i am indeed a pigment...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6051</td>\n",
       "      <td>7527ed21ee</td>\n",
       "      <td>I`m so unhappy being here. I hate it. It`s aff...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12712</td>\n",
       "      <td>2f6d9d1419</td>\n",
       "      <td>#stackeoverflow  http://bit.ly/13Nfk1</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2930</td>\n",
       "      <td>9afe83da6a</td>\n",
       "      <td>I`m completely exhausted thanks to yesterday. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2174</td>\n",
       "      <td>322b61740c</td>\n",
       "      <td>90 degrees, gross skies, and thunderstorms...p...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19780</th>\n",
       "      <td>11964</td>\n",
       "      <td>96674184d6</td>\n",
       "      <td>_X sorry about that.</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19781</th>\n",
       "      <td>21575</td>\n",
       "      <td>14ba250631</td>\n",
       "      <td>@_Bella_Cullen13 posted and its good</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19782</th>\n",
       "      <td>5390</td>\n",
       "      <td>ae19d98db6</td>\n",
       "      <td>:: holy kraut. I canNOT stay up this late. But...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19783</th>\n",
       "      <td>860</td>\n",
       "      <td>e9fdfb3211</td>\n",
       "      <td>with your bicycle?</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19784</th>\n",
       "      <td>15795</td>\n",
       "      <td>4b9f6d5ddb</td>\n",
       "      <td>Having a good...no great day already &amp; Dr.Mira...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19785 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0      textID  \\\n",
       "0           14951  7627d7ca56   \n",
       "1            6051  7527ed21ee   \n",
       "2           12712  2f6d9d1419   \n",
       "3            2930  9afe83da6a   \n",
       "4            2174  322b61740c   \n",
       "...           ...         ...   \n",
       "19780       11964  96674184d6   \n",
       "19781       21575  14ba250631   \n",
       "19782        5390  ae19d98db6   \n",
       "19783         860  e9fdfb3211   \n",
       "19784       15795  4b9f6d5ddb   \n",
       "\n",
       "                                                    text  label label_text  \n",
       "0      Hi : Found you through . i am indeed a pigment...      1    neutral  \n",
       "1      I`m so unhappy being here. I hate it. It`s aff...      0   negative  \n",
       "2                  #stackeoverflow  http://bit.ly/13Nfk1      1    neutral  \n",
       "3      I`m completely exhausted thanks to yesterday. ...      0   negative  \n",
       "4      90 degrees, gross skies, and thunderstorms...p...      2   positive  \n",
       "...                                                  ...    ...        ...  \n",
       "19780                               _X sorry about that.      0   negative  \n",
       "19781               @_Bella_Cullen13 posted and its good      2   positive  \n",
       "19782  :: holy kraut. I canNOT stay up this late. But...      0   negative  \n",
       "19783                                 with your bicycle?      1    neutral  \n",
       "19784  Having a good...no great day already & Dr.Mira...      2   positive  \n",
       "\n",
       "[19785 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datafeame of Primary data\n",
    "df = pd.read_csv(\"train_data.csv\")\n",
    "num_of_rows = len(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['found',\n",
       " 'indeed',\n",
       " 'pigment',\n",
       " 'imagination',\n",
       " 'unhappy',\n",
       " 'hate',\n",
       " 'affecting',\n",
       " 'everything',\n",
       " 'relationships',\n",
       " 'people',\n",
       " 'personality',\n",
       " 'attitude',\n",
       " '...',\n",
       " 'stackeoverflow',\n",
       " 'http',\n",
       " '//bit.ly/13nfk1',\n",
       " 'completely',\n",
       " 'exhausted',\n",
       " 'thanks',\n",
       " 'yesterday',\n",
       " 'home',\n",
       " 'yet',\n",
       " 'degrees',\n",
       " 'gross',\n",
       " 'skies',\n",
       " 'thunderstorms',\n",
       " 'perfect',\n",
       " 'match',\n",
       " 'mood',\n",
       " 'lol',\n",
       " 'cant',\n",
       " 'believe',\n",
       " 'stay',\n",
       " 'late',\n",
       " '.....',\n",
       " 'waking',\n",
       " 'work',\n",
       " 't-minus',\n",
       " '4hours',\n",
       " '_nobel',\n",
       " 'omg',\n",
       " 'mom',\n",
       " 'called',\n",
       " 'hes',\n",
       " 'gone',\n",
       " 'engaged',\n",
       " 'may',\n",
       " '1st',\n",
       " 'best',\n",
       " 'guy',\n",
       " 'could',\n",
       " 'happier',\n",
       " 'love',\n",
       " 'jay',\n",
       " 'mesaj',\n",
       " 'eroare',\n",
       " 'forbiddenyou',\n",
       " 'permission',\n",
       " 'access',\n",
       " '/photo/3180758.jpg',\n",
       " 'server',\n",
       " 'ghicit',\n",
       " 'sushi',\n",
       " 'joint',\n",
       " 'closed',\n",
       " 'still',\n",
       " 'nice',\n",
       " 'lunch',\n",
       " 'angel',\n",
       " 'whoops',\n",
       " 'start',\n",
       " 'trek',\n",
       " 'duh',\n",
       " 'think',\n",
       " 'beeen',\n",
       " 'onee',\n",
       " 'year',\n",
       " 'missed',\n",
       " 'jonaswebcast',\n",
       " 'sooo',\n",
       " 'thinks',\n",
       " 'belongs',\n",
       " 'elsewhere',\n",
       " 'say',\n",
       " 'dont',\n",
       " 'havin',\n",
       " 'stupid',\n",
       " 'stomach',\n",
       " 'pain',\n",
       " 'today',\n",
       " 'outing',\n",
       " 'sph',\n",
       " 'frenz',\n",
       " 'nvr',\n",
       " 'fails',\n",
       " 'dissappoint',\n",
       " 'haiz',\n",
       " 'leno',\n",
       " 'last',\n",
       " 'show',\n",
       " 'tonight',\n",
       " 'happy',\n",
       " 'mother',\n",
       " 'day',\n",
       " 'totally',\n",
       " 'felt',\n",
       " 'bad',\n",
       " 'haha',\n",
       " 'good',\n",
       " 'miss',\n",
       " 'guys',\n",
       " 'man',\n",
       " 'looking',\n",
       " 'packing',\n",
       " 'books',\n",
       " 'past',\n",
       " 'lives',\n",
       " 'feeling',\n",
       " 'kind',\n",
       " 'sentimental',\n",
       " 'hair',\n",
       " 'thankful',\n",
       " 'minute',\n",
       " 'doc',\n",
       " 'appointments',\n",
       " 'baby',\n",
       " 'girl',\n",
       " 'temp',\n",
       " '105.+',\n",
       " 'sitting',\n",
       " 'waitin',\n",
       " 'congrats',\n",
       " 'graduating',\n",
       " 'college',\n",
       " 'hard',\n",
       " 'back',\n",
       " 'morning-',\n",
       " 'straight',\n",
       " 'days',\n",
       " 'disneyland',\n",
       " 'tends',\n",
       " 'yes',\n",
       " 'got',\n",
       " 'lovey',\n",
       " 'returned',\n",
       " '1thing',\n",
       " 'led2',\n",
       " 'another',\n",
       " 'also',\n",
       " 'wants2',\n",
       " 'know',\n",
       " 'mad',\n",
       " 'bored',\n",
       " 'gon',\n",
       " 'carnival',\n",
       " 'get',\n",
       " 'pass',\n",
       " 'tomorrow',\n",
       " 'excited',\n",
       " 'cold',\n",
       " 'find',\n",
       " 'hoodie',\n",
       " 'mean',\n",
       " 'anna',\n",
       " 'right',\n",
       " 'need',\n",
       " 'make',\n",
       " 'sorry',\n",
       " 'hit',\n",
       " 'hay',\n",
       " 'came',\n",
       " 'goodnight',\n",
       " 'world',\n",
       " 'inhabit',\n",
       " 'much',\n",
       " 'longer',\n",
       " 'nkotb',\n",
       " 'block',\n",
       " 'party',\n",
       " 'bed',\n",
       " 'wondering',\n",
       " 'time',\n",
       " 'goddamn',\n",
       " 'bloody',\n",
       " 'stress',\n",
       " 'sending',\n",
       " 'body',\n",
       " 'haywire',\n",
       " 'please',\n",
       " 'pray',\n",
       " 'lord',\n",
       " 'knows',\n",
       " 'pointed',\n",
       " 'able',\n",
       " 'involved',\n",
       " 'convo',\n",
       " 'lack',\n",
       " 'iphone',\n",
       " 'cleaning',\n",
       " 'kitchen',\n",
       " 'watching',\n",
       " 'royals',\n",
       " 'play',\n",
       " 'morning',\n",
       " 'star',\n",
       " 'wars',\n",
       " '4th',\n",
       " 'like',\n",
       " 'workshop',\n",
       " 'ugh',\n",
       " 'til',\n",
       " 'end',\n",
       " 'june',\n",
       " 'instant',\n",
       " 'internet',\n",
       " 'marketing',\n",
       " 'empire',\n",
       " 'bonus',\n",
       " 'recoup',\n",
       " 'investment',\n",
       " 'hours',\n",
       " 'less',\n",
       " '//megaredpacket.com/',\n",
       " 'aff_id=7891',\n",
       " 'unny',\n",
       " 'taking',\n",
       " 'long',\n",
       " 'decide',\n",
       " 'wna',\n",
       " 'tan',\n",
       " 'getting',\n",
       " 'sacked',\n",
       " 'soon',\n",
       " 'mate',\n",
       " 'board',\n",
       " 'twitter',\n",
       " 'tweet',\n",
       " 'sounds',\n",
       " 'great',\n",
       " 'cried',\n",
       " 'would',\n",
       " 'travel',\n",
       " 'one',\n",
       " 'big',\n",
       " 'meets',\n",
       " 'sound',\n",
       " 'many',\n",
       " 'furs',\n",
       " 'art',\n",
       " 'lmao',\n",
       " '27th',\n",
       " 'awww',\n",
       " 'sucks',\n",
       " 'awesome',\n",
       " 'squeeky',\n",
       " 'left',\n",
       " 'though',\n",
       " 'soo',\n",
       " 'sick',\n",
       " 'wades',\n",
       " 'swings',\n",
       " 'uhh',\n",
       " 'ahhhh',\n",
       " '_truesdale',\n",
       " 'mess',\n",
       " 'atl',\n",
       " 'weeks',\n",
       " 'exams',\n",
       " 'carl',\n",
       " 'hope',\n",
       " 'difficult',\n",
       " 'well',\n",
       " 'btw',\n",
       " 'nathanfillion',\n",
       " 'trying',\n",
       " 'beat',\n",
       " 'flight',\n",
       " 'control',\n",
       " 'score',\n",
       " 'maybe',\n",
       " 'follow',\n",
       " 'sure',\n",
       " 'pick',\n",
       " 'chrisette',\n",
       " 'michele',\n",
       " 'new',\n",
       " 'album',\n",
       " 'epiphany',\n",
       " 'stores',\n",
       " '5th',\n",
       " 'promise',\n",
       " 'wanted',\n",
       " 'leave',\n",
       " '12:45',\n",
       " 'p.m.',\n",
       " 'showing',\n",
       " 'staff',\n",
       " 'retreat',\n",
       " 'mini',\n",
       " 'mothers',\n",
       " 'mommys',\n",
       " 'exspecially',\n",
       " 'mine',\n",
       " 'mommy',\n",
       " 'flickr',\n",
       " 'recognize',\n",
       " 'e-mail',\n",
       " 'addresses',\n",
       " 'log',\n",
       " 'joke',\n",
       " 'except',\n",
       " 'site',\n",
       " 'near',\n",
       " 'airport',\n",
       " 'dixon',\n",
       " 'lil',\n",
       " 'starbucks',\n",
       " 'gps',\n",
       " 'head',\n",
       " 'hehe',\n",
       " 'understand',\n",
       " 'anything',\n",
       " 'said',\n",
       " 'early',\n",
       " 'monday',\n",
       " 'rainy',\n",
       " 'thank',\n",
       " 'hoping',\n",
       " 'video',\n",
       " 'online',\n",
       " 'dinner',\n",
       " 'ready',\n",
       " 'going',\n",
       " 'life',\n",
       " 'seems',\n",
       " 'ideal',\n",
       " 'however',\n",
       " 'see',\n",
       " 'leaving',\n",
       " 'take',\n",
       " 'rain',\n",
       " 'check',\n",
       " 'eat',\n",
       " 'exactly',\n",
       " 'true',\n",
       " 'digest',\n",
       " 'todayisaprosperous',\n",
       " 'productive',\n",
       " 'thankujesus',\n",
       " 'beyeblessed',\n",
       " 'hee',\n",
       " 'flylady',\n",
       " 'deary',\n",
       " 'mins',\n",
       " 'enough',\n",
       " 'rescue',\n",
       " 'shall',\n",
       " 'come',\n",
       " 'later',\n",
       " 'mow',\n",
       " 'lawn',\n",
       " 'cool',\n",
       " 'outside',\n",
       " 'jealous',\n",
       " 'dallas',\n",
       " 'concert',\n",
       " 'tix',\n",
       " 'sold',\n",
       " 'naisee',\n",
       " 'lens',\n",
       " 'flares',\n",
       " 'arond',\n",
       " 'listening',\n",
       " 'really',\n",
       " 'catch',\n",
       " 'saying',\n",
       " 'tho',\n",
       " 'next',\n",
       " 'keep',\n",
       " 'handy',\n",
       " '//budurl.com/f9p5',\n",
       " 'funny',\n",
       " 'stuff',\n",
       " 'waiting',\n",
       " 'loooong',\n",
       " 'making',\n",
       " 'banana',\n",
       " 'bread',\n",
       " 'huh',\n",
       " 'moodle',\n",
       " 'intro',\n",
       " 'training',\n",
       " 'ages',\n",
       " 'since',\n",
       " 'done',\n",
       " 'bookings',\n",
       " 'someone',\n",
       " 'fly',\n",
       " 'reno',\n",
       " 'want',\n",
       " 'happen',\n",
       " '_bill2030',\n",
       " '_cullen',\n",
       " 'already',\n",
       " 'put',\n",
       " 'pics',\n",
       " '//twitpic.com/61oj0',\n",
       " 'woke',\n",
       " 'real',\n",
       " 'cause',\n",
       " 'krispy',\n",
       " 'kreme',\n",
       " 'doughnuts',\n",
       " 'audition',\n",
       " '-mhc-smooch',\n",
       " 'welcome',\n",
       " 'brazilians',\n",
       " 'meh',\n",
       " 'countries',\n",
       " 'absolutely',\n",
       " 'amazing',\n",
       " 'wolverine',\n",
       " 'beautiful',\n",
       " 'person',\n",
       " 'look',\n",
       " 'made',\n",
       " 'friends',\n",
       " 'around',\n",
       " 'usa',\n",
       " 'bike',\n",
       " 'across',\n",
       " 'trip',\n",
       " 'cartoons',\n",
       " 'simple',\n",
       " 'word',\n",
       " 'twi',\n",
       " 'front',\n",
       " 'clever',\n",
       " 'pune',\n",
       " 'michelle',\n",
       " 'hot',\n",
       " 'mama',\n",
       " 'chichis',\n",
       " 'grande',\n",
       " 'punchy',\n",
       " 'due',\n",
       " 'two',\n",
       " 'pager',\n",
       " 'events',\n",
       " 'nowhere',\n",
       " 'babe',\n",
       " 'ice',\n",
       " 'jobs',\n",
       " 'sigh',\n",
       " 'might',\n",
       " 'drive',\n",
       " 'chi-town',\n",
       " 'way',\n",
       " 'colorado',\n",
       " '....',\n",
       " 'open',\n",
       " 'everyone',\n",
       " 'glad/sad',\n",
       " 'me/my',\n",
       " 'badoptus',\n",
       " '//twitpic.com/4j9eo',\n",
       " 'looks',\n",
       " 'nights',\n",
       " 'poor',\n",
       " 'phone',\n",
       " 'nekkid',\n",
       " 'without',\n",
       " 'cover',\n",
       " '10pm',\n",
       " 'international',\n",
       " 'space',\n",
       " 'station',\n",
       " 'texas',\n",
       " 'least',\n",
       " 'britta',\n",
       " 'says',\n",
       " 'wants',\n",
       " 'working',\n",
       " 'fridaaaayyyyy',\n",
       " 'fkn',\n",
       " 'exam',\n",
       " 'bought',\n",
       " 'gift',\n",
       " 'finally',\n",
       " 'tada',\n",
       " 'prada',\n",
       " 'fragrance',\n",
       " 'meet',\n",
       " 'mum',\n",
       " 'fam',\n",
       " 'hahahah',\n",
       " 'course',\n",
       " 'nasty',\n",
       " 'display',\n",
       " 'picture',\n",
       " 'cheeks',\n",
       " 'bakery',\n",
       " 'crazy',\n",
       " 'williamsburg',\n",
       " 'support',\n",
       " 'infinite',\n",
       " 'boutiques',\n",
       " 'w/bad',\n",
       " 'clothes',\n",
       " 'decent',\n",
       " 'place',\n",
       " 'scone',\n",
       " 'officially',\n",
       " 'announced',\n",
       " 'luck',\n",
       " 'post',\n",
       " 'magical',\n",
       " 'village',\n",
       " 'weather',\n",
       " 'preview',\n",
       " '//pengpengsplace.blogspot.com',\n",
       " 'videos',\n",
       " 'miming',\n",
       " 'song',\n",
       " '_coleman',\n",
       " 'little',\n",
       " \"'fried\",\n",
       " 'fun',\n",
       " 'paul',\n",
       " 'friend',\n",
       " 'help',\n",
       " 'move',\n",
       " 'agree',\n",
       " 'nathan',\n",
       " 'eddie',\n",
       " 'brenden',\n",
       " 'chilling',\n",
       " 'flies',\n",
       " 'frd',\n",
       " 'love-hunting',\n",
       " 'mission',\n",
       " 'abt',\n",
       " 'bring',\n",
       " 'sexy',\n",
       " \"'yu\",\n",
       " 'yung',\n",
       " 'jacket',\n",
       " 'awwww',\n",
       " 'realize',\n",
       " 'bulletin',\n",
       " 'memories',\n",
       " 'tgc',\n",
       " 'old',\n",
       " 'remember',\n",
       " 'times',\n",
       " 'hey',\n",
       " 'change',\n",
       " 'account',\n",
       " 'even',\n",
       " 'tell',\n",
       " 'misses',\n",
       " 'dunhams',\n",
       " 'kayyy',\n",
       " 'yeah',\n",
       " 'always',\n",
       " 'talks',\n",
       " 'part',\n",
       " 'dog',\n",
       " 'dies',\n",
       " 'movie',\n",
       " 'seen',\n",
       " 'include',\n",
       " \"'follow\",\n",
       " 'friday',\n",
       " 'tactical',\n",
       " 'mistake',\n",
       " 'city',\n",
       " 'library',\n",
       " 'buying',\n",
       " 'milk',\n",
       " 'knew',\n",
       " 'ouch',\n",
       " 'thing',\n",
       " 'try',\n",
       " 'uni',\n",
       " 'demi',\n",
       " 'itd',\n",
       " 'covered',\n",
       " 'thats',\n",
       " 'paramore',\n",
       " 'dance',\n",
       " 'class',\n",
       " '2,697',\n",
       " 'views',\n",
       " 'yay',\n",
       " '//www.flickr.com/photos/bonassin/',\n",
       " 'sketch',\n",
       " 'final',\n",
       " 'catwalk',\n",
       " 'outfit',\n",
       " 'pink',\n",
       " 'dye',\n",
       " 'models',\n",
       " 'allowed',\n",
       " 'brand',\n",
       " 'playing',\n",
       " 'epicenter',\n",
       " 'july',\n",
       " 'jesse',\n",
       " 'lacey',\n",
       " 'test',\n",
       " 'env2',\n",
       " 'sometimes',\n",
       " 'hurts',\n",
       " 'pets',\n",
       " 'talk',\n",
       " 'dude',\n",
       " 'tried',\n",
       " 'vegetarian',\n",
       " 'lasted',\n",
       " 'months',\n",
       " 'vegan',\n",
       " 'intense',\n",
       " 'epicfail',\n",
       " 'joseph',\n",
       " 'liao',\n",
       " 'whole',\n",
       " 'season',\n",
       " 'behind',\n",
       " 'pulled',\n",
       " 'breakfast',\n",
       " 'sausage',\n",
       " 'hopefully',\n",
       " 'sleeps',\n",
       " '_city',\n",
       " 'fob',\n",
       " 'luv',\n",
       " 'frineds',\n",
       " 'followers',\n",
       " '~ellen~',\n",
       " 'wishes',\n",
       " 'someday',\n",
       " 'adventure',\n",
       " 'kinda',\n",
       " 'worried',\n",
       " 'bus',\n",
       " 'cos',\n",
       " 'oyster',\n",
       " 'broken',\n",
       " 'addicted',\n",
       " 'pet',\n",
       " 'society',\n",
       " 'app',\n",
       " 'facebook',\n",
       " 'jus',\n",
       " 'sittn',\n",
       " 'thinkin',\n",
       " 'wow',\n",
       " 'boy',\n",
       " 'smh',\n",
       " 'bout',\n",
       " 'sunday',\n",
       " 'r.i.p',\n",
       " 'von',\n",
       " 'musashi',\n",
       " 'rep.',\n",
       " 'kitaro',\n",
       " 'typical',\n",
       " 'google',\n",
       " 'docs',\n",
       " 'pages',\n",
       " 'remind',\n",
       " 'kansai',\n",
       " 'scene',\n",
       " 'whatever',\n",
       " 'current',\n",
       " 'issue',\n",
       " 'nothing',\n",
       " 'holes',\n",
       " 'bbc1',\n",
       " 'doesnt',\n",
       " 'burn',\n",
       " 'lampions',\n",
       " 'wishing',\n",
       " 'happiness',\n",
       " 'ahead',\n",
       " \"'the\",\n",
       " 'fifth',\n",
       " 'element',\n",
       " 'makes',\n",
       " \"'super\",\n",
       " 'green',\n",
       " 'lot',\n",
       " 'funnier',\n",
       " 'john',\n",
       " 'wear',\n",
       " 'mask',\n",
       " 'gloves',\n",
       " 'swine',\n",
       " 'flu',\n",
       " '//yfrog.com/02mxjj',\n",
       " 'nostalgic',\n",
       " 'sad',\n",
       " 'feel',\n",
       " 'feels',\n",
       " 'dice',\n",
       " 'heroes',\n",
       " '//plurk.com/p/svhvh',\n",
       " 'ever',\n",
       " 'fab',\n",
       " 'broke',\n",
       " 'cost',\n",
       " 'quite',\n",
       " 'bit',\n",
       " 'flying',\n",
       " 'pizza',\n",
       " 'china',\n",
       " 'fusion',\n",
       " 'idea',\n",
       " 'experiment',\n",
       " 'leavin',\n",
       " 'tha',\n",
       " 'studio.made',\n",
       " 'music',\n",
       " 'pleased',\n",
       " 'wit',\n",
       " 'turnout',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'brother',\n",
       " 'saw',\n",
       " 'bruce',\n",
       " 'walking',\n",
       " 'sidewalk',\n",
       " 'sanibel',\n",
       " 'used',\n",
       " 'live',\n",
       " 'pine',\n",
       " 'blast',\n",
       " 'garcia',\n",
       " 'bend-',\n",
       " 'learned',\n",
       " 'lesson',\n",
       " 'night',\n",
       " 'thought',\n",
       " 'throw',\n",
       " 'fashion',\n",
       " 'lonely',\n",
       " 'there/fly',\n",
       " 'snuggle',\n",
       " 'replied',\n",
       " 'ppl',\n",
       " 'thx',\n",
       " 'snl',\n",
       " 'sunburn',\n",
       " 'went',\n",
       " 'york',\n",
       " 'smith',\n",
       " '//twitpic.com/669l2',\n",
       " '_songs',\n",
       " 'especially',\n",
       " 'summer',\n",
       " 'boring',\n",
       " 'begin',\n",
       " 'wan',\n",
       " 'maui',\n",
       " 'liloven',\n",
       " 'lady',\n",
       " 'bummer',\n",
       " 'stayed',\n",
       " 'almost',\n",
       " 'reading',\n",
       " \"'rules\",\n",
       " 'book',\n",
       " 'worth',\n",
       " 'sleep',\n",
       " 'deprivation',\n",
       " 'comes',\n",
       " 'fixd',\n",
       " 'emails',\n",
       " 'hacked',\n",
       " 'although',\n",
       " 'seemed',\n",
       " 'suspicious',\n",
       " 'wish',\n",
       " 'incredible',\n",
       " 'phenomenal',\n",
       " 'amazingly',\n",
       " 'talented',\n",
       " 'singer',\n",
       " 'gleneagles',\n",
       " 'champagne',\n",
       " 'receptions',\n",
       " 'alone',\n",
       " 'tea',\n",
       " 'rubbish',\n",
       " 'ive',\n",
       " 'housework',\n",
       " 'yep',\n",
       " 'sang',\n",
       " 'flashlight',\n",
       " 'tag',\n",
       " 'games',\n",
       " 'dark',\n",
       " 'told',\n",
       " 'chance',\n",
       " 'gettin',\n",
       " 'job',\n",
       " '.omgggg',\n",
       " 'seeing',\n",
       " 'proto-furries',\n",
       " 'acen',\n",
       " 'gh5',\n",
       " 'boston',\n",
       " 'melting',\n",
       " 'house',\n",
       " 'shade',\n",
       " 'fair',\n",
       " 'ops',\n",
       " 'queen',\n",
       " 'homework',\n",
       " '_is_one84',\n",
       " 'suck',\n",
       " 'states',\n",
       " 'enjoy',\n",
       " 'ride',\n",
       " 'alter',\n",
       " 'blockbuster',\n",
       " 'week',\n",
       " 'zealand',\n",
       " \"'wolverine\",\n",
       " 'followed',\n",
       " \"'star\",\n",
       " 'thursday',\n",
       " 'finished',\n",
       " 'mirrors',\n",
       " 'liked',\n",
       " 'didnt',\n",
       " 'ten',\n",
       " 'give',\n",
       " '//g',\n",
       " 'occured',\n",
       " 'visit',\n",
       " 'ventana',\n",
       " 'los',\n",
       " 'cielos',\n",
       " 'foundation',\n",
       " 'pushed',\n",
       " 'bck',\n",
       " 'kids',\n",
       " 'wait',\n",
       " 'wks',\n",
       " 'hear',\n",
       " 'fan',\n",
       " 'dead',\n",
       " 'gerbil',\n",
       " 'fabulous',\n",
       " 'coffee',\n",
       " 'obviously',\n",
       " 'cup',\n",
       " 'large',\n",
       " 'embrace',\n",
       " 'deficiency',\n",
       " 'parents',\n",
       " 'putting',\n",
       " 'upset',\n",
       " 'fireworks',\n",
       " 'kboom',\n",
       " 'second',\n",
       " 'preceded',\n",
       " '2008',\n",
       " 'hungry',\n",
       " 'food',\n",
       " 'steal',\n",
       " 'following',\n",
       " 'mya',\n",
       " 'rains',\n",
       " 'pours',\n",
       " '//twitpic.com/4jhp8',\n",
       " 'plan',\n",
       " 'happens',\n",
       " 'gets',\n",
       " '//tinyurl.com/55hq2o',\n",
       " 'sweet',\n",
       " 'sleeping',\n",
       " 'lmfaoooo',\n",
       " 'showwwww',\n",
       " 'arrrgh',\n",
       " 'men',\n",
       " '......',\n",
       " 'face',\n",
       " '........',\n",
       " 'mon',\n",
       " 'josh',\n",
       " 'excuse',\n",
       " 'ahhh',\n",
       " 'others',\n",
       " 'hoppusday',\n",
       " 'etc',\n",
       " 'spazz',\n",
       " 'irish',\n",
       " 'wedding',\n",
       " 'visiting',\n",
       " 'grandparents',\n",
       " 'manhattan',\n",
       " 'dropping',\n",
       " 'sister',\n",
       " 'tired',\n",
       " '_shines92',\n",
       " 'aww',\n",
       " 'annoying',\n",
       " 'psh',\n",
       " 'open-houses',\n",
       " 'cowboy',\n",
       " 'hat',\n",
       " 'pic',\n",
       " 'smile',\n",
       " 'japan',\n",
       " 'despite',\n",
       " 'fantastic',\n",
       " 'november',\n",
       " 'super',\n",
       " 'soak',\n",
       " 'stars',\n",
       " 'weekend',\n",
       " 'sry',\n",
       " 'calls',\n",
       " 'fell',\n",
       " 'asleep',\n",
       " 'boredom',\n",
       " 'sunrise',\n",
       " 'enjoyable',\n",
       " 'actually',\n",
       " 'mcdonalds',\n",
       " 'combo',\n",
       " 'wendys',\n",
       " 'sundae',\n",
       " 'puke',\n",
       " 'material',\n",
       " 'bank',\n",
       " 'teller',\n",
       " 'definitely',\n",
       " 'hitting',\n",
       " 'interested',\n",
       " 'dear',\n",
       " 'laughed',\n",
       " 'add',\n",
       " '//profiles.friendster.com/americanidolislove',\n",
       " 'friendster',\n",
       " 'hahah',\n",
       " 'chill',\n",
       " 'usual',\n",
       " 'decided',\n",
       " 'sweetie',\n",
       " 'rcb',\n",
       " 'trashes',\n",
       " 'mumbai',\n",
       " 'indians',\n",
       " '3rd',\n",
       " 'episode',\n",
       " 'jonas',\n",
       " 'emo',\n",
       " 'moment',\n",
       " 'bye',\n",
       " 'pirated',\n",
       " 'yaaaay',\n",
       " 'aced',\n",
       " 'history',\n",
       " 'hahaha',\n",
       " 'ugly',\n",
       " 'fourth',\n",
       " 'something',\n",
       " 'read',\n",
       " '//is.gd/jmjb',\n",
       " 'bass',\n",
       " 'drum',\n",
       " 'heads',\n",
       " 'breaking',\n",
       " 'equals',\n",
       " 'bust',\n",
       " 'needs',\n",
       " 'tickets',\n",
       " '15th',\n",
       " 'repeat',\n",
       " 'seconds',\n",
       " 'game',\n",
       " 'dal-den',\n",
       " 'officials',\n",
       " 'anyway',\n",
       " 'likes',\n",
       " 'cheerios',\n",
       " 'scones',\n",
       " 'spend',\n",
       " 'experiement',\n",
       " 'warhammer',\n",
       " '40k',\n",
       " 'marines',\n",
       " '360',\n",
       " 'ps3',\n",
       " 'gamers',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess data into clean data\n",
    "def preprocess(text):\n",
    "    if isinstance(text, str):\n",
    "        word_list = word_tokenize(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_word_list = [word.lower() for word in word_list if word.lower() not in stop_words and len(word) > 2]\n",
    "        \n",
    "        return filtered_word_list\n",
    "    \n",
    "    return []\n",
    "\n",
    "# iterate text columns from df and using preprocess function\n",
    "total_words = list()\n",
    "for i in range(num_of_rows):\n",
    "    text_to_process = df[\"text\"][i]\n",
    "    result = list(preprocess(text_to_process))\n",
    "    total_words.extend(result)\n",
    "\n",
    "# Find the number of repetitions of words\n",
    "count_data = collections.Counter(total_words)\n",
    "unique_words = list(count_data.keys())\n",
    "unique_words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phase2: word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_23288\\1850941540.py:41: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  row_sums = df1.sum(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>found</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>indeed</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hate</td>\n",
       "      <td>5</td>\n",
       "      <td>193</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>everything</td>\n",
       "      <td>31</td>\n",
       "      <td>25</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>people</td>\n",
       "      <td>72</td>\n",
       "      <td>94</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>got</td>\n",
       "      <td>250</td>\n",
       "      <td>251</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>another</td>\n",
       "      <td>50</td>\n",
       "      <td>57</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>also</td>\n",
       "      <td>49</td>\n",
       "      <td>41</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>know</td>\n",
       "      <td>175</td>\n",
       "      <td>191</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>mad</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  positive  negative  neutral\n",
       "0         found        32        35       49\n",
       "1        indeed        12         4        4\n",
       "5          hate         5       193       46\n",
       "7    everything        31        25       36\n",
       "9        people        72        94      102\n",
       "..          ...       ...       ...      ...\n",
       "142         got       250       251      388\n",
       "147     another        50        57       85\n",
       "148        also        49        41       53\n",
       "150        know       175       191      308\n",
       "151         mad         4        25       12\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iterate text columns from df and using preprocess function\n",
    "total_words = list()\n",
    "positive_words = list()\n",
    "negative_words = list()\n",
    "neutral_words = list()\n",
    "\n",
    "for i in range(num_of_rows):\n",
    "    text_to_process = df[\"text\"][i]\n",
    "    result = list(preprocess(text_to_process))\n",
    "    total_words.extend(result)\n",
    "    \n",
    "    #convert words into positive and negative and neutral categories\n",
    "    if df.iloc[i]['label_text'] == 'positive':\n",
    "        positive_words.extend(result)\n",
    "        \n",
    "    elif df.iloc[i]['label_text'] == 'negative':\n",
    "        negative_words.extend(result)\n",
    "        \n",
    "    if df.iloc[i]['label_text'] == 'neutral':\n",
    "        neutral_words.extend(result)\n",
    "\n",
    "# Find the number of repetitions of words\n",
    "count_data_total = collections.Counter(total_words)\n",
    "count_data_positive = collections.Counter(positive_words)\n",
    "count_data_negative = collections.Counter(negative_words)\n",
    "count_data_neutral = collections.Counter(neutral_words)\n",
    "\n",
    "#create dataframe with 4 columns words, positive , negative, neutral\n",
    "words = count_data_total.keys()\n",
    "pos = [count_data_positive[i] for i in words]\n",
    "neg = [count_data_negative[i] for i in words]\n",
    "neutral = [count_data_neutral[i] for i in words]\n",
    "\n",
    "#show df\n",
    "dict = {'word': words, 'positive': pos, 'negative': neg, 'neutral':neutral} \n",
    "df1 = pd.DataFrame(dict)\n",
    "\n",
    "df2 = df1[df1['word'].isin(unique_words)]\n",
    "\n",
    "# Drop rows where the sum is less than 10\n",
    "row_sums = df1.sum(axis=1)\n",
    "df2 = df1[row_sums >= 10]\n",
    "df2.head(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phase3: caculating the probabilities of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_23288\\3392659924.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df3 = df3.append(temp, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive_probability</th>\n",
       "      <th>negative_probability</th>\n",
       "      <th>neutral_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>0.001271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.006229</td>\n",
       "      <td>0.001195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.000940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002022</td>\n",
       "      <td>0.003050</td>\n",
       "      <td>0.002618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.006953</td>\n",
       "      <td>0.008091</td>\n",
       "      <td>0.009888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>0.002186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.001373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.004876</td>\n",
       "      <td>0.006165</td>\n",
       "      <td>0.007854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.000330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     positive_probability  negative_probability  neutral_probability\n",
       "0                0.000914              0.001156             0.001271\n",
       "1                0.000360              0.000161             0.000127\n",
       "5                0.000166              0.006229             0.001195\n",
       "7                0.000887              0.000835             0.000940\n",
       "9                0.002022              0.003050             0.002618\n",
       "..                    ...                   ...                  ...\n",
       "142              0.006953              0.008091             0.009888\n",
       "147              0.001413              0.001862             0.002186\n",
       "148              0.001385              0.001349             0.001373\n",
       "150              0.004876              0.006165             0.007854\n",
       "151              0.000139              0.000835             0.000330\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df2.copy()\n",
    "\n",
    "# Calculate the sum of each column to find the capacity of each class\n",
    "sum_positive = df3['positive'].sum()\n",
    "sum_negative = df3['negative'].sum()\n",
    "sum_neutral = df3['neutral'].sum()\n",
    "N_class = [sum_positive, sum_negative, sum_neutral] #[45074, 39758, 52176]\n",
    "\n",
    "# Create a dictionary with 'word' as a separate key\n",
    "temp = {'word': 'N_class', 'positive': sum_positive, 'negative': sum_negative, 'neutral': sum_neutral}\n",
    "df3 = df3.append(temp, ignore_index=True)\n",
    "\n",
    "# Calculate the probabilities\n",
    "positive_probability = (df2['positive'] + 1) / (N_class[0] + len(df2))\n",
    "negative_probability = (df2['negative'] + 1) / (N_class[1] + len(df2))\n",
    "neutral_probability = (df2['neutral'] + 1) / (N_class[2] + len(df2))\n",
    "\n",
    "df4 = pd.DataFrame()\n",
    "df4[\"positive_probability\"] = positive_probability\n",
    "df4[\"negative_probability\"] = negative_probability\n",
    "df4[\"neutral_probability\"] = neutral_probability\n",
    "# Display the updated DataFrame\n",
    "df4.head(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phase 4: get the log prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
